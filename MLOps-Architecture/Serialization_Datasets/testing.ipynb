{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d44922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------- Library Imports ------------------- \"\"\"\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6664367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pub_ip_addr_and_port(name):\n",
    "    temp = !kubectl get services --all-namespaces\n",
    "    node_port = \"\"\n",
    "    for line in temp:\n",
    "        if name in line:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 6:\n",
    "                node_port = parts[5].split(':')[1].split('/')[0]  # Extract NodePort\n",
    "    temp = !kubectl get pods -o wide | findstr /R {name}\n",
    "    node_name = temp[0].split()[6]  # Node name is the 7th column (index 6)\n",
    "    temp = !kubectl get nodes -o wide | findstr /R {node_name}\n",
    "    pub_ip = temp[0].split()[6]  # Public IP is the\n",
    "    return f\"{pub_ip}:{node_port}\"\n",
    "\n",
    "def get_inf_ipaddress():\n",
    "    # Get all pods in all namespaces in JSON format\n",
    "    result = subprocess.run(\n",
    "        [\"kubectl\", \"get\", \"pods\", \"-A\", \"-o\", \"json\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    pods_info = json.loads(result.stdout)\n",
    "    pod_ips = []\n",
    "    for item in pods_info.get(\"items\", []):\n",
    "        metadata = item.get(\"metadata\", {})\n",
    "        name = metadata.get(\"name\", \"\")\n",
    "        status = item.get(\"status\", {})\n",
    "        pod_ip = status.get(\"podIP\")\n",
    "        if \"api-inferencia\" in name and pod_ip:\n",
    "            pod_ips.append(pod_ip)\n",
    "    return pod_ips\n",
    "\n",
    "def copy_file_to_docker(file_path, docker_path):\n",
    "    \"\"\"Copy a file from local to GCP VM and then into the Docker container.\"\"\"\n",
    "    filename = file_path.split(\"/\")[-1] if \"/\" in file_path else file_path.split(\"\\\\\")[-1]\n",
    "    app_data_path = !dir \"%APPDATA%\"\n",
    "    app_data_path = app_data_path[3].split()[-1]\n",
    "    app_data_path = app_data_path.replace(\"\\\\\", \"/\")\n",
    "    username = app_data_path.split(\"/\")[2]\n",
    "    # Copy file to VM\n",
    "    !gcloud compute scp --zone=us-central1-a {file_path} load-testing-instance:/home/{username}\n",
    "    # Copy file from VM to Docker container\n",
    "    copy_command = f\"docker cp /home/{username}/{filename} load-testing-container:{docker_path}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{copy_command}\"\n",
    "\n",
    "def run_docker_command(inner_command):\n",
    "    \"\"\"\n",
    "    Run a command inside load-testing-container on the GCP VM via SSH using docker exec.\n",
    "    \"\"\"\n",
    "    docker_exec_template = \"docker exec load-testing-container bash -c '{cmd}'\"\n",
    "    docker_command = docker_exec_template.format(cmd=inner_command)\n",
    "    ssh_command = f\"gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\\\"{docker_command}\\\"\"\n",
    "    !{ssh_command}\n",
    "\n",
    "def copy_dir_to_docker(dir_path, docker_path):\n",
    "    \"\"\"Copy a directory from local to GCP VM and then into the Docker container.\"\"\"\n",
    "    dir_name = dir_path.split(\"/\")[-1] if \"/\" in dir_path else dir_path.split(\"\\\\\")[-1]\n",
    "    username = dir_path.split(\"/\")[2] if \"/\" in dir_path else dir_path.split(\"\\\\\")[-2]\n",
    "    # Clean up the VM directory \n",
    "    rm_command = f\"sudo rm -rf /home/{username}/{dir_name}\"  # Clean up the VM directory after copying\n",
    "    docker_rm_command = f\"docker exec load-testing-container rm -rf {docker_path}{dir_name}\"  # Clean up the Docker directory\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{rm_command}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{docker_rm_command}\"\n",
    "    # Copy directory to VM\n",
    "    print(f\"gcloud compute scp --zone=us-central1-a --recurse {dir_path} load-testing-instance:/home/{username}/\")\n",
    "    !gcloud compute scp --zone=us-central1-a --recurse {dir_path} load-testing-instance:/home/{username}/\n",
    "    # Copy directory from VM to Docker container\n",
    "    copy_command = f\"docker cp /home/{username}/{dir_name} load-testing-container:{docker_path}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{copy_command}\"\n",
    "\n",
    "def run_vm_command(command):\n",
    "    \"\"\"\n",
    "    Run a command on the GCP VM via SSH.\n",
    "    \"\"\"\n",
    "    ssh_command = f\"gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\\\"{command}\\\"\"\n",
    "    !{ssh_command}\n",
    "\n",
    "def copy_docker_file_to_local(docker_path, local_path):\n",
    "    \"\"\"Copy a file or directory from the Docker container to local.\"\"\"\n",
    "    # Get username from VM\n",
    "    app_data_path = !dir \"%APPDATA%\"\n",
    "    app_data_path = app_data_path[3].split()[-1]\n",
    "    app_data_path = app_data_path.replace(\"\\\\\", \"/\")\n",
    "    username = app_data_path.split(\"/\")[2]\n",
    "    filename = docker_path.split(\"/\")[-1] if \"/\" in docker_path else docker_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    # Copy file from Docker container to VM home directory\n",
    "    copy_command = f\"docker cp load-testing-container:{docker_path} /home/{username}/{filename}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{copy_command}\"\n",
    "    # Copy file from VM to local\n",
    "    copy_command = f\"gcloud compute scp --zone=us-central1-a load-testing-instance:/home/{username}/{filename} {local_path}\"\n",
    "    !{copy_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687a34cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Service IP: http://34.10.166.82:32002\n",
      "Inference Service IP: http://34.56.104.203:32001\n"
     ]
    }
   ],
   "source": [
    "# Get the public IP address and port for the update and inference services\n",
    "ip_address_update = \"http://\" + get_pub_ip_addr_and_port(\"upd\")\n",
    "ip_address_inference = \"http://\" + get_pub_ip_addr_and_port(\"inf\")\n",
    "# ip_address_update = \"http://localhost/update-service\"\n",
    "# ip_address_inference = \"http://localhost/inference-service\"\n",
    "print(f\"Update Service IP: {ip_address_update}\")\n",
    "print(f\"Inference Service IP: {ip_address_inference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c2537e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pods loaded with response: 200\n",
      "Counter reset with response: 200\n",
      "Rate configured with response: 200\n",
      "MLflow URI configured with response: 200\n",
      "Kafka URI configured with response: 200\n",
      "Experiment AGR_a-HT-1new-0 loaded with response: 200\n"
     ]
    }
   ],
   "source": [
    "# List of experiments to run\n",
    "experiments = [\n",
    "    # [\"AGR_g-HT-old-1000\", \"AGR_g_real_test.csv\"],\n",
    "    # [\"AGR_g-HT-old-2000\", \"AGR_g_real_test.csv\"],\n",
    "    # [\"AGR_g-HT-old-10000\", \"AGR_g_real_test.csv\"],\n",
    "    # [\"AGR_g-HT-1new-0\", \"AGR_g_real_test.csv\"],\n",
    "    # [\"AGR_a-HT-old-2000\", \"AGR_a_real_test.csv\"],\n",
    "    # [\"AGR_a-HT-old-10000\", \"AGR_a_real_test.csv\"],\n",
    "    # [\"AGR_a-HT-old-1000\", \"AGR_a_real_test.csv\"],\n",
    "    # [\"AGR_a-HT-old-1000\", \"AGR_a_real_test.csv\"],\n",
    "    # [\"AGR_a-HT-old-1000\", \"AGR_a_real_test.csv\"],\n",
    "    [\"AGR_a-HT-1new-0\", \"AGR_a_real_test.csv\"],\n",
    "]\n",
    "loads = 1000\n",
    "experiment = experiments[0]\n",
    "experiment_name = experiment[0]  # Use the first experiment name\n",
    "dataset_name = experiment[1]  # Use the first dataset name\n",
    "mlflow_service_uri = \"http://mlflow:5000\"\n",
    "kafka_service_uri = \"kafka-service.kafka:9092\"\n",
    "inf_ipaddress = get_inf_ipaddress()\n",
    "inf_load_urls = [\"http://\" + ip + \":5001/load\" for ip in inf_ipaddress]\n",
    "model_update_rate = experiment[0].split(\"-\")[-1]  # Extracting the rate from the experiment name \n",
    "\n",
    "# FOR INFERENCE KAFKA\n",
    "# url_kafka = f'{ip_address_inference}/kafka-uri'\n",
    "# r_kafka = requests.post(url_kafka, json=(kafka_service_uri))\n",
    "# print(f\"Kafka URI INFERENCE configured with response: {r_kafka.status_code}\")\n",
    "# print('Curl for INFERENCE KAFKA:')\n",
    "# print(f\"curl -X POST {url_kafka} -H 'Content-Type: application/json' -d '{json.dumps(kafka_service_uri)}'\")\n",
    "\n",
    "# FOR INFERENCE MLFLOW\n",
    "# url_kafka = f'{ip_address_inference}/mlflow-uri'\n",
    "# r_kafka = requests.post(url_kafka, json=(mlflow_service_uri))\n",
    "# print(f\"MLFLOW URI INFERENCE configured with response: {r_kafka.status_code}\")\n",
    "# print('Curl for INFERENCE MLFLOW:')\n",
    "# print(f\"curl -X POST {ip_address_inference}/mlflow-uri -H 'Content-Type: application/json' -d '{json.dumps(mlflow_service_uri)}'\")\n",
    "\n",
    "# GIVE INFERENCE PODS\n",
    "update_to_pods = f\"{ip_address_update}/pods\"\n",
    "r_inf_pods = requests.post(update_to_pods, json=inf_load_urls)\n",
    "print(f\"Pods loaded with response: {r_inf_pods.status_code}\")\n",
    "# print('Curl for UPDATE PODS:')\n",
    "# print(f\"curl -X POST {update_to_pods} -H 'Content-Type: application/json' -d '{json.dumps(inf_load_urls)}'\")\n",
    "\n",
    "\n",
    "# # Reset Counter\n",
    "url_count = f'{ip_address_update}/count'\n",
    "r_count = requests.post(url_count, json=(0))\n",
    "print(f\"Counter reset with response: {r_count.status_code}\")\n",
    "# print('Curl for UPDATE COUNT:')\n",
    "# print(f\"curl -X POST {url_count} -H 'Content-Type: application/json' -d '0'\")\n",
    "\n",
    "\n",
    "# # Configure Rate\n",
    "url_rate = f'{ip_address_update}/rate'\n",
    "r_rate = requests.post(url_rate, json=(model_update_rate))\n",
    "print(f\"Rate configured with response: {r_rate.status_code}\")\n",
    "# print('Curl for UPDATE RATE:')\n",
    "# print(f\"curl -X POST {url_rate} -H 'Content-Type: application/json' -d '{json.dumps(model_update_rate)}'\")\n",
    "\n",
    "\n",
    "url_mlflow = f'{ip_address_update}/mlflow-uri'\n",
    "r_mlflow = requests.post(url_mlflow, json=(mlflow_service_uri))\n",
    "print(f\"MLflow URI configured with response: {r_mlflow.status_code}\")\n",
    "# print('Curl for UPDATE MLFLOW:')\n",
    "# print(f\"curl -X POST {url_mlflow} -H 'Content-Type: application/json' -d '{json.dumps(mlflow_service_uri)}'\")\n",
    "\n",
    "\n",
    "url_kafka = f'{ip_address_update}/kafka-uri'\n",
    "r_kafka = requests.post(url_kafka, json=(kafka_service_uri))\n",
    "print(f\"Kafka URI configured with response: {r_kafka.status_code}\")\n",
    "# print('Curl for UPDATE KAFKA:')\n",
    "# print(f\"curl -X POST {url_kafka} -H 'Content-Type: application/json' -d '{json.dumps(kafka_service_uri)}'\")\n",
    "\n",
    "\n",
    "url_load = f'{ip_address_update}/load'\n",
    "exp_name = experiment[0]  # Use the first experiment name\n",
    "r_load = requests.post(url_load, json=(exp_name))\n",
    "print(f\"Experiment {exp_name} loaded with response: {r_load.status_code}\")\n",
    "# print('Curl for UPDATE LOAD:')\n",
    "# print(f\"curl -X POST {url_load} -H 'Content-Type: application/json' -d '{json.dumps(exp_name)}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9beb7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: Prediction = 0.02326669978609882\n",
      "Row 1: Prediction = 0.9108201652401648\n",
      "Row 2: Prediction = 0.02326669978609882\n",
      "Row 3: Prediction = 0.0\n",
      "Row 4: Prediction = 0.0\n",
      "Row 5: Prediction = 0.7165180280047907\n",
      "Row 6: Prediction = 0.02326669978609882\n",
      "Row 7: Prediction = 0.02326669978609882\n",
      "Row 8: Prediction = 0.02326669978609882\n",
      "Row 9: Prediction = 0.815006027942381\n",
      "Row 10: Prediction = 0.0\n",
      "Row 11: Prediction = 0.02326669978609882\n",
      "Row 12: Prediction = 0.715050910355845\n",
      "Row 13: Prediction = 0.02326669978609882\n",
      "Row 14: Prediction = 0.02326669978609882\n",
      "Row 15: Prediction = 0.02326669978609882\n",
      "Row 16: Prediction = 0.0\n",
      "Row 17: Prediction = 0.0\n",
      "Row 18: Prediction = 0.9197849396351291\n",
      "Row 19: Prediction = 0.0\n",
      "Row 20: Prediction = 0.02326669978609882\n",
      "Row 21: Prediction = 0.5180240554112743\n",
      "Row 22: Prediction = 0.02326669978609882\n",
      "Row 23: Prediction = 0.0\n",
      "Row 24: Prediction = 0.9052542451937076\n",
      "Row 25: Prediction = 0.7444504820951255\n",
      "Row 26: Prediction = 0.02326669978609882\n",
      "Row 27: Prediction = 0.02326669978609882\n",
      "Row 28: Prediction = 0.0\n",
      "Row 29: Prediction = 0.02326669978609882\n",
      "Row 30: Prediction = 0.893293546653259\n",
      "Row 31: Prediction = 0.02326669978609882\n",
      "Row 32: Prediction = 0.8827261225977696\n",
      "Row 33: Prediction = 0.02326669978609882\n",
      "Row 34: Prediction = 0.02326669978609882\n",
      "Row 35: Prediction = 0.868537601583033\n",
      "Row 36: Prediction = 0.6351700744649569\n",
      "Row 37: Prediction = 0.02326669978609882\n",
      "Row 38: Prediction = 0.9008462472108186\n",
      "Row 39: Prediction = 0.02326669978609882\n",
      "Row 40: Prediction = 0.7570773543830955\n",
      "Row 41: Prediction = 0.02326669978609882\n",
      "Row 42: Prediction = 0.02326669978609882\n",
      "Row 43: Prediction = 0.6714156773771693\n",
      "Row 44: Prediction = 0.02326669978609882\n",
      "Row 45: Prediction = 0.0\n",
      "Row 46: Prediction = 0.445753023440043\n",
      "Row 47: Prediction = 0.02326669978609882\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m data = row.tolist()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Send data to your prediction endpoint\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mip_address_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/predict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     12\u001b[39m     prediction_result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.aulia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# [SKIP] Simple testing of the inference service\n",
    "df = pd.read_csv(r'C:\\Users\\muhammad.aulia\\large-scale-online-learning\\Datasets\\real_usage\\AGR_g_real_test.csv')\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    # Convert row to list and prepare data\n",
    "    data = row.tolist()\n",
    "    \n",
    "    # Send data to your prediction endpoint\n",
    "    response = requests.post(ip_address_inference + \"/predict\", json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        prediction_result = response.json()\n",
    "        print(f\"Row {idx}: Prediction = {prediction_result}\")\n",
    "    else:\n",
    "        print(f\"Row {idx}: Error - {response.status_code}\")\n",
    "    count += 1\n",
    "    if count >= 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcea1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LOCUST\n",
    "# locust -f MLOps-Architecture/Serialization_Datasets/locust-testing.py\n",
    "# locust -f MLOps-Architecture/Serialization_Datasets/locust-testing.py --users 1 --spawn-rate 1 --headless --csv=result_testing\n",
    "# [BACKGROUND RUN]\n",
    "# nohup locust -f MLOps-Architecture/Serialization_Datasets/locust-testing.py --users 1 --spawn-rate 1 --headless --csv=result_testing > locust.log 2>&1 &\n",
    "\n",
    "import re\n",
    "\n",
    "substitutions = {\n",
    "    # C:\\Users\\adilm\\OneDrive\\Documents\\large-scale-online-learning\\MLOps-Architecture\\Serialization_Datasets\\testing.ipynb\n",
    "    # C:\\Users\\adilm\\OneDrive\\Documents\\large-scale-online-learning\\Datasets\\real_usage\n",
    "    # go to real_usage using relative path\n",
    "    \"dataset_dir\": \"Datasets/real_usage/\",\n",
    "    \"file_name\": dataset_name,\n",
    "    # \"host1\": \"http://localhost/inference-service\"  \n",
    "    \"host1\": ip_address_inference\n",
    "}\n",
    "\n",
    "for var, value in substitutions.items():\n",
    "    # For string values, ensure quotes in the replacement\n",
    "    replacement = f'{var} = \"{value}\"'\n",
    "    # Use sed to replace the line in locust-test.py\n",
    "    file_path = 'locust-testing.py'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    for var, value in substitutions.items():\n",
    "        # Replace lines like: var = \"old_value\"\n",
    "        content = re.sub(\n",
    "            rf'^{var}\\s*=.*$', \n",
    "            f'{var} = \"{value}\"', \n",
    "            content, \n",
    "            flags=re.MULTILINE\n",
    "        )\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SKIP] EXPORT VARIABLES\n",
    "variables = {\n",
    "    \"experiments\": str(experiments),\n",
    "    \"experiment\": str(experiment),\n",
    "    \"mlflow_service_uri\": mlflow_service_uri,\n",
    "    \"kafka_service_uri\": kafka_service_uri,\n",
    "    \"inf_ipaddress\": str(inf_ipaddress),\n",
    "    \"inf_load_urls\": str(inf_load_urls),\n",
    "    \"model_update_rate\": model_update_rate\n",
    "}\n",
    "\n",
    "with open(\"etc/variables.txt\", \"w\") as f:\n",
    "    for key, value in variables.items():\n",
    "        f.write(f\"{key}={value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37a53d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "locust-testing.py         | 3 kB |   3.4 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    }
   ],
   "source": [
    "# COPY FILE LOCUST OR CHECK KAFKA LAG \n",
    "copy_file_to_docker(\"locust-testing.py\", \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/\")\n",
    "# copy_file_to_docker(\"check-kafka-lag.py\", \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca55b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcloud compute scp --zone=us-central1-a --recurse C:/Users/muhammad.aulia/AppData/Roaming/gcloud load-testing-instance:/home/muhammad.aulia/\n",
      "\n",
      ".last_opt_in_prompt.yaml  | 0 kB |   0.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      ".last_survey_prompt.yaml  | 0 kB |   0.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      ".last_update_check.json   | 0 kB |   0.2 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "access_tokens.db          | 4 kB |   4.0 kB/s | ETA: 00:00:02 |  33%\n",
      "access_tokens.db          | 12 kB |  12.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "active_config             | 0 kB |   0.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "application_default_crede | 0 kB |   0.4 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "resource.cache            | 4 kB |   4.0 kB/s | ETA: 00:00:02 |  33%\n",
      "resource.cache            | 12 kB |  12.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "config_default            | 0 kB |   0.1 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "credentials.db            | 4 kB |   4.0 kB/s | ETA: 00:00:02 |  33%\n",
      "credentials.db            | 12 kB |  12.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "default_configs.db        | 4 kB |   4.0 kB/s | ETA: 00:00:02 |  33%\n",
      "default_configs.db        | 12 kB |  12.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "gce                       | 0 kB |   0.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "hidden_gcloud_config_univ | 4 kB |   4.0 kB/s | ETA: 00:00:02 |  33%\n",
      "hidden_gcloud_config_univ | 12 kB |  12.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      ".boto                     | 0 kB |   0.2 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "adc.json                  | 0 kB |   0.3 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "12.43.53.236191.log       | 4 kB |   4.0 kB/s | ETA: 00:00:00 |  80%\n",
      "12.43.53.236191.log       | 4 kB |   5.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "12.43.59.529786.log       | 4 kB |   4.0 kB/s | ETA: 00:00:00 |  78%\n",
      "12.43.59.529786.log       | 5 kB |   5.1 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pscp: C:/Users/muhammad.aulia/AppData/Roaming/gcloud/logs/2025.08.23/12.44.05.829656.log: Cannot open file\n",
      "\n",
      "ERROR: (gcloud.compute.scp) [C:\\Users\\muhammad.aulia\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\sdk\\pscp.exe] exited with return code [1].\n"
     ]
    }
   ],
   "source": [
    "# COPY GOOGLE CLOUD\n",
    "\n",
    "app_data_path = !dir \"%APPDATA%\"\n",
    "!rmdir /s /q %APPDATA%\\gcloud\\logs\n",
    "app_data_path = app_data_path[3].split()[-1]\n",
    "app_data_path = app_data_path.replace(\"\\\\\", \"/\")\n",
    "copy_dir_to_docker(f'{app_data_path}/gcloud', \"/root/.config/gcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check copy file\n",
    "path = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets\"\n",
    "run_docker_command(\"cat \" + path + \"/locust-testing.py\")\n",
    "# run_docker_command(\"cat \" + path + \"/check-kafka-lag.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f81f67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for two-node-cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              READY   STATUS    RESTARTS   AGE\n",
      "api-inferencia-685b667b65-2tj5d   1/1     Running   0          71m\n",
      "api-inferencia-685b667b65-7l4nq   1/1     Running   0          71m\n",
      "api-inferencia-685b667b65-g9xck   1/1     Running   0          71m\n",
      "api-inferencia-685b667b65-xch42   1/1     Running   0          71m\n",
      "api-update-d44fddd4c-5ch7f        1/1     Running   0          71m\n",
      "mlflow-6cf7458897-r5t6w           1/1     Running   0          71m\n"
     ]
    }
   ],
   "source": [
    "# Check GCloud\n",
    "projects = !gcloud config get-value project\n",
    "set_project_command = f\"gcloud config set project {projects[0]}\"\n",
    "get_kubectl_command = \"gcloud container clusters get-credentials two-node-cluster --zone us-central1-a\"\n",
    "run_docker_command(set_project_command)\n",
    "run_docker_command(get_kubectl_command)\n",
    "run_docker_command(\"kubectl get pods\")\n",
    "# run_docker_command(\"gcloud config get-value project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df908d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/app/message_log.csv': No such file or directory\n",
      "command terminated with exit code 1\n",
      "rm: cannot remove '/app/model_upload_latency.csv': No such file or directory\n",
      "command terminated with exit code 1\n",
      "rm: cannot remove '/app/for_auc.csv': No such file or directory\n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "# Delete all copied files in kubernetes and RUN THE TEST\n",
    "pods = !kubectl get pods | findstr /R \"upd\"\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/message_log.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/model_upload_latency.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/for_auc.csv\n",
    "pods = !kubectl get pods | findstr /R \"inf\"\n",
    "for pod in pods:\n",
    "    # Delete all files matching load_model*.csv at the end of the file in /app, suppress error if not found\n",
    "    !kubectl exec {pods[0].split()[0]} -- bash -c \"rm -f load_model*.gz\"\n",
    "    !kubectl exec {pod.split()[0]} -- bash -c \"rm -f load_model*.csv\"\n",
    "    !kubectl exec {pod.split()[0]} -- bash -c \"rm -f api_inference_*.log\"\n",
    "run_docker_command(\"rm -rf /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv\")\n",
    "run_docker_command(\"rm -rf /app/large-scale-online-learning/locust.log\")\n",
    "\n",
    "# RUN THE TEST\n",
    "locust_path = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/locust-testing.py\"\n",
    "check_kafka_lag_dir_path = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets\"\n",
    "locust_command = f\"locust -f {locust_path} --users {loads} --spawn-rate {loads} --headless --csv=result_testing\"\n",
    "run_docker_command(f\"cd {check_kafka_lag_dir_path} && nohup python3 check-kafka-lag.py > kafka_lag.log 2>&1 &\")\n",
    "run_docker_command(f\"cd /app/large-scale-online-learning/ && source ../.python-venv/bin/activate && nohup {locust_command} > locust.log 2>&1 &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11eb532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST     /predict                                                                      494029 494029(100.00%) |    751      12    1881    710 | 1323.80     1323.80\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated                                                                    494029 494029(100.00%) |    751      12    1881    710 | 1323.80     1323.80\n",
      "\n",
      "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "POST     /predict                                                                      494029 494029(100.00%) |    751      12    1881    710 | 1323.80     1323.80\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated                                                                    494029 494029(100.00%) |    751      12    1881    710 | 1323.80     1323.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF LOAD TESTING IS WORKING\n",
    "# run_docker_command(\"cat /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv | tail -n 10\")\n",
    "run_docker_command(\"cat /app/large-scale-online-learning/locust.log | tail -n 10\")\n",
    "\n",
    "# clean kafka lag log\n",
    "# run_docker_command(\"rm -rf /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv\")\n",
    "# run_docker_command(\"rm -rf /app/large-scale-online-learning/locust.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1b878c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP THE PROCESS KAFKA LAG AND LOCUST TESTING\n",
    "run_docker_command(\"pkill -f check-kafka-lag.py\")\n",
    "run_docker_command(\"pkill -f locust-testing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d6114da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "A subdirectory or file experiment-results\\AGR_g-HT-old-10000-30000VU\\inference-results already exists.\n",
      "tar: Removing leading `/' from member names\n",
      "A subdirectory or file experiment-results\\AGR_g-HT-old-10000-30000VU\\inference-results already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "A subdirectory or file experiment-results\\AGR_g-HT-old-10000-30000VU\\inference-results already exists.\n",
      "tar: Removing leading `/' from member names\n",
      "tar: Removing leading `/' from member names\n",
      "tar: Removing leading `/' from member names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "\n",
      "for_auc.tar.gz            | 32 kB |  32.0 kB/s | ETA: 00:01:56 |   0%\n",
      "for_auc.tar.gz            | 128 kB |  64.0 kB/s | ETA: 00:00:56 |   3%\n",
      "for_auc.tar.gz            | 576 kB | 192.0 kB/s | ETA: 00:00:16 |  15%\n",
      "for_auc.tar.gz            | 1056 kB | 264.0 kB/s | ETA: 00:00:10 |  28%\n",
      "for_auc.tar.gz            | 1824 kB | 364.8 kB/s | ETA: 00:00:05 |  48%\n",
      "for_auc.tar.gz            | 2336 kB | 389.3 kB/s | ETA: 00:00:03 |  62%\n",
      "for_auc.tar.gz            | 2880 kB | 411.4 kB/s | ETA: 00:00:02 |  76%\n",
      "for_auc.tar.gz            | 3712 kB | 464.0 kB/s | ETA: 00:00:00 |  98%\n",
      "for_auc.tar.gz            | 3754 kB | 469.4 kB/s | ETA: 00:00:00 | 100%\n",
      "tar: Removing leading `/' from member names\n",
      "\n",
      "message_log.tar.gz        | 32 kB |  32.0 kB/s | ETA: 00:10:28 |   0%\n",
      "message_log.tar.gz        | 64 kB |  32.0 kB/s | ETA: 00:10:27 |   0%\n",
      "message_log.tar.gz        | 128 kB |  42.7 kB/s | ETA: 00:07:49 |   0%\n",
      "message_log.tar.gz        | 160 kB |  40.0 kB/s | ETA: 00:08:19 |   0%\n",
      "message_log.tar.gz        | 192 kB |  38.4 kB/s | ETA: 00:08:39 |   0%\n",
      "message_log.tar.gz        | 256 kB |  42.7 kB/s | ETA: 00:07:46 |   1%\n",
      "message_log.tar.gz        | 320 kB |  45.7 kB/s | ETA: 00:07:13 |   1%\n",
      "message_log.tar.gz        | 384 kB |  48.0 kB/s | ETA: 00:06:51 |   1%\n",
      "message_log.tar.gz        | 480 kB |  53.3 kB/s | ETA: 00:06:08 |   2%\n",
      "message_log.tar.gz        | 576 kB |  57.6 kB/s | ETA: 00:05:39 |   2%\n",
      "message_log.tar.gz        | 704 kB |  64.0 kB/s | ETA: 00:05:03 |   3%\n",
      "message_log.tar.gz        | 832 kB |  69.3 kB/s | ETA: 00:04:38 |   4%\n",
      "message_log.tar.gz        | 960 kB |  73.8 kB/s | ETA: 00:04:19 |   4%\n",
      "message_log.tar.gz        | 1120 kB |  80.0 kB/s | ETA: 00:03:57 |   5%\n",
      "message_log.tar.gz        | 1280 kB |  85.3 kB/s | ETA: 00:03:41 |   6%\n",
      "message_log.tar.gz        | 1376 kB |  86.0 kB/s | ETA: 00:03:38 |   6%\n",
      "message_log.tar.gz        | 1600 kB |  94.1 kB/s | ETA: 00:03:17 |   7%\n",
      "message_log.tar.gz        | 1728 kB |  96.0 kB/s | ETA: 00:03:11 |   8%\n",
      "message_log.tar.gz        | 1920 kB | 101.1 kB/s | ETA: 00:03:00 |   9%\n",
      "message_log.tar.gz        | 2080 kB | 104.0 kB/s | ETA: 00:02:53 |  10%\n",
      "message_log.tar.gz        | 2304 kB | 109.7 kB/s | ETA: 00:02:42 |  11%\n",
      "message_log.tar.gz        | 2432 kB | 110.5 kB/s | ETA: 00:02:40 |  12%\n",
      "message_log.tar.gz        | 2624 kB | 114.1 kB/s | ETA: 00:02:33 |  13%\n",
      "message_log.tar.gz        | 2912 kB | 121.3 kB/s | ETA: 00:02:22 |  14%\n",
      "message_log.tar.gz        | 3232 kB | 129.3 kB/s | ETA: 00:02:10 |  16%\n",
      "message_log.tar.gz        | 3488 kB | 134.2 kB/s | ETA: 00:02:04 |  17%\n",
      "message_log.tar.gz        | 3744 kB | 138.7 kB/s | ETA: 00:01:58 |  18%\n",
      "message_log.tar.gz        | 4096 kB | 146.3 kB/s | ETA: 00:01:49 |  20%\n",
      "message_log.tar.gz        | 4416 kB | 152.3 kB/s | ETA: 00:01:43 |  21%\n",
      "message_log.tar.gz        | 4800 kB | 160.0 kB/s | ETA: 00:01:35 |  23%\n",
      "message_log.tar.gz        | 5216 kB | 168.3 kB/s | ETA: 00:01:28 |  25%\n",
      "message_log.tar.gz        | 5536 kB | 173.0 kB/s | ETA: 00:01:24 |  27%\n",
      "message_log.tar.gz        | 6016 kB | 182.3 kB/s | ETA: 00:01:17 |  29%\n",
      "message_log.tar.gz        | 6368 kB | 187.3 kB/s | ETA: 00:01:13 |  31%\n",
      "message_log.tar.gz        | 6816 kB | 194.7 kB/s | ETA: 00:01:08 |  33%\n",
      "message_log.tar.gz        | 7168 kB | 199.1 kB/s | ETA: 00:01:05 |  35%\n",
      "message_log.tar.gz        | 7424 kB | 200.6 kB/s | ETA: 00:01:03 |  36%\n",
      "message_log.tar.gz        | 7744 kB | 203.8 kB/s | ETA: 00:01:00 |  38%\n",
      "message_log.tar.gz        | 8000 kB | 205.1 kB/s | ETA: 00:00:59 |  39%\n",
      "message_log.tar.gz        | 8288 kB | 207.2 kB/s | ETA: 00:00:57 |  41%\n",
      "message_log.tar.gz        | 8448 kB | 206.0 kB/s | ETA: 00:00:56 |  41%\n",
      "message_log.tar.gz        | 8640 kB | 205.7 kB/s | ETA: 00:00:55 |  42%\n",
      "message_log.tar.gz        | 8800 kB | 204.7 kB/s | ETA: 00:00:55 |  43%\n",
      "message_log.tar.gz        | 9056 kB | 205.8 kB/s | ETA: 00:00:53 |  44%\n",
      "message_log.tar.gz        | 9248 kB | 205.5 kB/s | ETA: 00:00:53 |  45%\n",
      "message_log.tar.gz        | 9408 kB | 204.5 kB/s | ETA: 00:00:52 |  46%\n",
      "message_log.tar.gz        | 9504 kB | 202.2 kB/s | ETA: 00:00:52 |  47%\n",
      "message_log.tar.gz        | 9664 kB | 201.3 kB/s | ETA: 00:00:52 |  47%\n",
      "message_log.tar.gz        | 9760 kB | 199.2 kB/s | ETA: 00:00:52 |  48%\n",
      "message_log.tar.gz        | 9952 kB | 199.0 kB/s | ETA: 00:00:51 |  49%\n",
      "message_log.tar.gz        | 10112 kB | 198.3 kB/s | ETA: 00:00:50 |  50%\n",
      "message_log.tar.gz        | 10240 kB | 196.9 kB/s | ETA: 00:00:50 |  50%\n",
      "message_log.tar.gz        | 10400 kB | 196.2 kB/s | ETA: 00:00:49 |  51%\n",
      "message_log.tar.gz        | 10560 kB | 195.6 kB/s | ETA: 00:00:49 |  52%\n",
      "message_log.tar.gz        | 10656 kB | 193.7 kB/s | ETA: 00:00:48 |  52%\n",
      "message_log.tar.gz        | 10784 kB | 192.6 kB/s | ETA: 00:00:48 |  53%\n",
      "message_log.tar.gz        | 10912 kB | 191.4 kB/s | ETA: 00:00:48 |  54%\n",
      "message_log.tar.gz        | 11008 kB | 189.8 kB/s | ETA: 00:00:48 |  54%\n",
      "message_log.tar.gz        | 11136 kB | 188.7 kB/s | ETA: 00:00:47 |  55%\n",
      "message_log.tar.gz        | 11296 kB | 188.3 kB/s | ETA: 00:00:47 |  56%\n",
      "message_log.tar.gz        | 11392 kB | 186.8 kB/s | ETA: 00:00:46 |  56%\n",
      "message_log.tar.gz        | 11520 kB | 185.8 kB/s | ETA: 00:00:46 |  57%\n",
      "message_log.tar.gz        | 11776 kB | 186.9 kB/s | ETA: 00:00:44 |  58%\n",
      "message_log.tar.gz        | 11904 kB | 186.0 kB/s | ETA: 00:00:44 |  59%\n",
      "message_log.tar.gz        | 12160 kB | 187.1 kB/s | ETA: 00:00:42 |  60%\n",
      "message_log.tar.gz        | 12480 kB | 189.1 kB/s | ETA: 00:00:40 |  61%\n",
      "message_log.tar.gz        | 12832 kB | 191.5 kB/s | ETA: 00:00:38 |  63%\n",
      "message_log.tar.gz        | 12864 kB | 189.2 kB/s | ETA: 00:00:38 |  63%\n",
      "message_log.tar.gz        | 14464 kB | 209.6 kB/s | ETA: 00:00:27 |  71%\n",
      "message_log.tar.gz        | 14752 kB | 210.7 kB/s | ETA: 00:00:25 |  73%\n",
      "message_log.tar.gz        | 15552 kB | 219.0 kB/s | ETA: 00:00:20 |  77%\n",
      "message_log.tar.gz        | 16192 kB | 224.9 kB/s | ETA: 00:00:17 |  80%\n",
      "message_log.tar.gz        | 16544 kB | 226.6 kB/s | ETA: 00:00:15 |  82%\n",
      "message_log.tar.gz        | 16928 kB | 228.8 kB/s | ETA: 00:00:14 |  84%\n",
      "message_log.tar.gz        | 17056 kB | 227.4 kB/s | ETA: 00:00:13 |  84%\n",
      "message_log.tar.gz        | 17344 kB | 228.2 kB/s | ETA: 00:00:12 |  86%\n",
      "message_log.tar.gz        | 17632 kB | 229.0 kB/s | ETA: 00:00:10 |  87%\n",
      "message_log.tar.gz        | 17824 kB | 228.5 kB/s | ETA: 00:00:10 |  88%\n",
      "message_log.tar.gz        | 18112 kB | 229.3 kB/s | ETA: 00:00:08 |  89%\n",
      "message_log.tar.gz        | 18368 kB | 229.6 kB/s | ETA: 00:00:07 |  91%\n",
      "message_log.tar.gz        | 18624 kB | 229.9 kB/s | ETA: 00:00:06 |  92%\n",
      "message_log.tar.gz        | 18816 kB | 229.5 kB/s | ETA: 00:00:05 |  93%\n",
      "message_log.tar.gz        | 19008 kB | 229.0 kB/s | ETA: 00:00:04 |  94%\n",
      "message_log.tar.gz        | 19200 kB | 228.6 kB/s | ETA: 00:00:04 |  95%\n",
      "message_log.tar.gz        | 19360 kB | 227.8 kB/s | ETA: 00:00:03 |  96%\n",
      "message_log.tar.gz        | 19552 kB | 227.3 kB/s | ETA: 00:00:02 |  97%\n",
      "message_log.tar.gz        | 19744 kB | 226.9 kB/s | ETA: 00:00:01 |  98%\n",
      "message_log.tar.gz        | 19968 kB | 226.9 kB/s | ETA: 00:00:00 |  99%\n",
      "message_log.tar.gz        | 20128 kB | 226.2 kB/s | ETA: 00:00:00 |  99%\n",
      "message_log.tar.gz        | 20146 kB | 226.4 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "model_upload_latency.tar. | 0 kB |   0.8 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "tar: Removing leading `/' from member names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "tar: Removing leading `/' from member names\n",
      "\n",
      "result_testing_stats_hist | 32 kB |  32.0 kB/s | ETA: 00:00:07 |  11%\n",
      "result_testing_stats_hist | 64 kB |  64.0 kB/s | ETA: 00:00:03 |  23%\n",
      "result_testing_stats_hist | 269 kB | 269.8 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "result_testing_exceptions | 0 kB |   0.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "result_testing_failures.c | 0 kB |   0.3 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "result_testing_stats.csv  | 0 kB |   0.6 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "kafka_lag_log.csv         | 0 kB |   0.5 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    }
   ],
   "source": [
    "# GET INFERENCE RESULTS | GET UPDATE RESULTS | GET MLFLOW RESULTS | GET LOAD TESTING RESULTS\n",
    "inference_pods = !kubectl get pods | findstr /R \"inf\"\n",
    "if inference_pods:\n",
    "    filenames = !kubectl exec {inference_pods[0].split()[0]} -- ls | findstr /R \"load_model\"\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    !kubectl exec {inference_pods[0].split()[0]} -- tar -czf /app/{filename}.tar.gz /app/{filename}\n",
    "    experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\inference-results\"\n",
    "    !mkdir {experiment_folder}\n",
    "    !kubectl cp default/{inference_pods[0].split()[0]}:/app/{filename}.tar.gz {experiment_folder}/{filename}.tar.gz\n",
    "# GET UPDATE RESULTS\n",
    "pods = !kubectl get pods | findstr /R \"upd\"\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /app/for_auc.tar.gz /app/for_auc.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /app/message_log.tar.gz /app/message_log.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /app/model_upload_latency.tar.gz /app/model_upload_latency.csv\n",
    "experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\update-results\"\n",
    "!mkdir {experiment_folder}\n",
    "run_docker_command(f\"kubectl cp default/{pods[0].split()[0]}:/app/for_auc.tar.gz app/for_auc.tar.gz\")\n",
    "copy_docker_file_to_local(\"/app/for_auc.tar.gz\", f\"{experiment_folder}/for_auc.tar.gz\")\n",
    "run_docker_command(f\"kubectl cp default/{pods[0].split()[0]}:/app/message_log.tar.gz app/message_log.tar.gz\")\n",
    "copy_docker_file_to_local(\"/app/message_log.tar.gz\", f\"{experiment_folder}/message_log.tar.gz\")\n",
    "run_docker_command(f\"kubectl cp default/{pods[0].split()[0]}:/app/model_upload_latency.tar.gz app/model_upload_latency.tar.gz\")\n",
    "copy_docker_file_to_local(\"/app/model_upload_latency.tar.gz\", f\"{experiment_folder}/model_upload_latency.tar.gz\")\n",
    "# GET MLFLOW RESULT\n",
    "pods = !kubectl get pods | findstr /R \"mlflow\"\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /mlartifacts/mlflow-results.tar.gz /mlartifacts/1\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /mlartifacts/mlflow-db.tar.gz /mlartifacts/mlflow.db\n",
    "experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\mlflow-results\"\n",
    "!kubectl cp default/{pods[0].split()[0]}:/mlartifacts/mlflow-results.tar.gz {experiment_folder}/mlflow-results.tar.gz\n",
    "!kubectl cp default/{pods[0].split()[0]}:/mlartifacts/mlflow-db.tar.gz {experiment_folder}/mlflow-db.tar.gz \n",
    "# GET LOAD TESTING RESULT\n",
    "docker_path = \"/app/large-scale-online-learning/\"\n",
    "docker_path_kafka_log = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/\"\n",
    "experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\load-test-results\"\n",
    "\n",
    "import os\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_stats_history.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_exceptions.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_failures.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_stats.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path_kafka_log}kafka_lag_log.csv\", experiment_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33c82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 114M\n",
      "-rw-rw-rw- 1 root root 577K Aug 19 04:35 AGR_a_first_train.csv\n",
      "-rw-rw-rw- 1 root root 577K Aug 19 04:35 AGR_g_first_train.csv\n",
      "-rw-rw-rw- 1 root root 9.1K Aug 19 04:34 API_update_V8.1.py\n",
      "-rw-r--r-- 1 root root 456K Aug 19 06:02 api_update.log\n",
      "-rw-r--r-- 1 root root  16M Aug 19 06:03 for_auc.csv\n",
      "-rw-r--r-- 1 root root 3.6M Aug 19 06:16 for_auc.tar.gz\n",
      "drwx------ 2 root root  16K Aug 19 02:02 lost+found\n",
      "-rw-r--r-- 1 root root  74M Aug 19 06:03 message_log.csv\n",
      "-rw-r--r-- 1 root root  20M Aug 19 06:16 message_log.tar.gz\n",
      "-rw-r--r-- 1 root root  12K Aug 19 06:03 model_upload_latency.csv\n",
      "-rw-r--r-- 1 root root 3.2K Aug 19 06:16 model_upload_latency.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# check file size in /app\n",
    "pods = !kubectl get pods | findstr /R \"upd\"\n",
    "!kubectl exec {pods[0].split()[0]} -- ls -lh /app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "438edcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all copied files in kubernetes\n",
    "pods = !kubectl get pods | findstr /R \"upd\"\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/message_log.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/model_upload_latency.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/for_auc.csv\n",
    "pods = !kubectl get pods | findstr /R \"inf\"\n",
    "for pod in pods:\n",
    "    # Delete all files matching load_model*.csv at the end of the file in /app, suppress error if not found\n",
    "    !kubectl exec {pods[0].split()[0]} -- bash -c \"rm -f load_model*.gz\"\n",
    "    !kubectl exec {pod.split()[0]} -- bash -c \"rm -f load_model*.csv\"\n",
    "    !kubectl exec {pod.split()[0]} -- bash -c \"rm -f api_inference_*.log\"\n",
    "run_docker_command(\"rm -rf /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv\")\n",
    "run_docker_command(\"rm -rf /app/large-scale-online-learning/locust.log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
