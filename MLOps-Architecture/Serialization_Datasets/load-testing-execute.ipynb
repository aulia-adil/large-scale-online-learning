{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955f5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file_to_docker(file_path, docker_path):\n",
    "    \"\"\"Copy a file from local to GCP VM and then into the Docker container.\"\"\"\n",
    "    filename = file_path.split(\"/\")[-1] if \"/\" in file_path else file_path.split(\"\\\\\")[-1]\n",
    "    app_data_path = !dir \"%APPDATA%\"\n",
    "    app_data_path = app_data_path[3].split()[-1]\n",
    "    app_data_path = app_data_path.replace(\"\\\\\", \"/\")\n",
    "    username = app_data_path.split(\"/\")[2]\n",
    "    # Copy file to VM\n",
    "    !gcloud compute scp --zone=us-central1-a {file_path} load-testing-instance:/home/{username}\n",
    "    # Copy file from VM to Docker container\n",
    "    copy_command = f\"docker cp /home/{username}/{filename} load-testing-container:{docker_path}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{copy_command}\"\n",
    "\n",
    "def run_docker_command(inner_command):\n",
    "    \"\"\"\n",
    "    Run a command inside load-testing-container on the GCP VM via SSH using docker exec.\n",
    "    \"\"\"\n",
    "    docker_exec_template = \"docker exec load-testing-container bash -c '{cmd}'\"\n",
    "    docker_command = docker_exec_template.format(cmd=inner_command)\n",
    "    ssh_command = f\"gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\\\"{docker_command}\\\"\"\n",
    "    !{ssh_command}\n",
    "\n",
    "def copy_dir_to_docker(dir_path, docker_path):\n",
    "    \"\"\"Copy a directory from local to GCP VM and then into the Docker container.\"\"\"\n",
    "    dir_name = dir_path.split(\"/\")[-1] if \"/\" in dir_path else dir_path.split(\"\\\\\")[-1]\n",
    "    username = dir_path.split(\"/\")[2] if \"/\" in dir_path else dir_path.split(\"\\\\\")[-2]\n",
    "    # Clean up the VM directory \n",
    "    rm_command = f\"sudo rm -rf /home/{username}/{dir_name}\"  # Clean up the VM directory after copying\n",
    "    docker_rm_command = f\"docker exec load-testing-container rm -rf {docker_path}{dir_name}\"  # Clean up the Docker directory\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{rm_command}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{docker_rm_command}\"\n",
    "    # Copy directory to VM\n",
    "    print(f\"gcloud compute scp --zone=us-central1-a --recurse {dir_path} load-testing-instance:/home/{username}/\")\n",
    "    !gcloud compute scp --zone=us-central1-a --recurse {dir_path} load-testing-instance:/home/{username}/\n",
    "    # Copy directory from VM to Docker container\n",
    "    copy_command = f\"docker cp /home/{username}/{dir_name} load-testing-container:{docker_path}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{copy_command}\"\n",
    "\n",
    "def run_vm_command(command):\n",
    "    \"\"\"\n",
    "    Run a command on the GCP VM via SSH.\n",
    "    \"\"\"\n",
    "    ssh_command = f\"gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\\\"{command}\\\"\"\n",
    "    !{ssh_command}\n",
    "\n",
    "def copy_docker_file_to_local(docker_path, local_path):\n",
    "    \"\"\"Copy a file or directory from the Docker container to local.\"\"\"\n",
    "    # Get username from VM\n",
    "    app_data_path = !dir \"%APPDATA%\"\n",
    "    app_data_path = app_data_path[3].split()[-1]\n",
    "    app_data_path = app_data_path.replace(\"\\\\\", \"/\")\n",
    "    username = app_data_path.split(\"/\")[2]\n",
    "    filename = docker_path.split(\"/\")[-1] if \"/\" in docker_path else docker_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    # Copy file from Docker container to VM home directory\n",
    "    copy_command = f\"docker cp load-testing-container:{docker_path} /home/{username}/{filename}\"\n",
    "    !gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"{copy_command}\"\n",
    "    # Copy file from VM to local\n",
    "    copy_command = f\"gcloud compute scp --zone=us-central1-a load-testing-instance:/home/{username}/{filename} {local_path}\"\n",
    "    !{copy_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ddec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ZONE           SIZE_GB  TYPE         STATUS\n",
      "load-testing-instance-disk  us-central1-a  30       pd-standard  READY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created [https://www.googleapis.com/compute/v1/projects/fast-learner-project/zones/us-central1-a/disks/load-testing-instance-disk].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS\n",
      "load-testing-instance  us-central1-a  e2-standard-8               10.128.0.17  34.41.103.73  RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created [https://www.googleapis.com/compute/v1/projects/fast-learner-project/zones/us-central1-a/instances/load-testing-instance].\n"
     ]
    }
   ],
   "source": [
    "# [SKIP] RUN LOAD TESTING VM\n",
    "vm_name = \"load-testing-instance\"\n",
    "snapshot_name = \"load-testing-instance-snapshot\"\n",
    "zone = \"us-central1-a\"\n",
    "!gcloud compute disks create {vm_name}-disk --source-snapshot={snapshot_name} --zone={zone}\n",
    "!gcloud compute instances create {vm_name} --zone={zone} --disk=name={vm_name}-disk,boot=yes,auto-delete=yes --machine-type=e2-standard-8\n",
    "# !gcloud compute firewall-rules create allow-all --direction=INGRESS --priority=1000 --network=default --action=ALLOW --rules=all --source-ranges=0.0.0.0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9937c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VM load-testing-instance is not running. Current status: ERROR: (gcloud.compute.instances.describe) Could not fetch resource:\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF VM IS RUNNING\n",
    "vm_name = \"load-testing-instance\"\n",
    "vm_status = !gcloud compute instances describe {vm_name} --zone={zone} --format=\"get(status)\"\n",
    "if vm_status[0] == \"RUNNING\":\n",
    "    print(f\"VM {vm_name} is running.\")\n",
    "else:\n",
    "    print(f\"VM {vm_name} is not running. Current status: {vm_status[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141ff51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting instance(s) load-testing-instance...\n",
      "...........done.\n",
      "Updated [https://compute.googleapis.com/compute/v1/projects/fast-learner-project/zones/us-central1-a/instances/load-testing-instance].\n",
      "Instance internal IP is 10.128.0.17\n",
      "Instance external IP is 34.69.115.89\n"
     ]
    }
   ],
   "source": [
    "# [SKIP] Include in user docker so it run docker command without sudo\n",
    "# run_vm_command(\"sudo usermod -aG docker $USER\")\n",
    "\n",
    "# Restart vm\n",
    "# !gcloud compute instances stop load-testing-instance --zone=us-central1-a\n",
    "!gcloud compute instances start load-testing-instance --zone=us-central1-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66828c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "run_vm_command(\"docker ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49879c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load-testing-container\n"
     ]
    }
   ],
   "source": [
    "# RUN LOAD TESTING CONTAINER\n",
    "# docker_command = \"docker start -d --name load-testing-container auliadil/load-testing-rodrigues:v1 tail -f /dev/null\"\n",
    "docker_command = \"docker start load-testing-container\"\n",
    "gcloud_template = \"gcloud compute ssh load-testing-instance --zone=us-central1-a\"\n",
    "!{gcloud_template} --command=\"{docker_command}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e265685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container is running.\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF CONTAINER IS RUNNING\n",
    "container_status = !gcloud compute ssh load-testing-instance --zone={zone} --command=\"docker ps -q --filter 'name=load-testing-container'\"\n",
    "if container_status:\n",
    "    print(\"Container is running.\")\n",
    "else:\n",
    "    print(\"Container is not running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1f6f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "locust-testing.py         | 3 kB |   3.4 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    }
   ],
   "source": [
    "# COPY FILE\n",
    "copy_file_to_docker(\"locust-testing.py\", \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/\")\n",
    "# copy_file_to_docker(\"check-kafka-lag.py\", \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/\")\n",
    "\n",
    "# app_data_path = !dir \"%APPDATA%\"\n",
    "# app_data_path = app_data_path[3].split()[-1]\n",
    "# app_data_path = app_data_path.replace(\"\\\\\", \"/\")\n",
    "# copy_dir_to_docker(f'{app_data_path}/gcloud', \"/root/.config/gcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdddd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check copy file\n",
    "path = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets\"\n",
    "run_docker_command(\"cat \" + path + \"/locust-testing.py\")\n",
    "# run_docker_command(\"cat \" + path + \"/check-kafka-lag.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36600818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for two-node-cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              READY   STATUS    RESTARTS   AGE\n",
      "api-inferencia-7dc65cbb55-4mxqf   1/1     Running   0          135m\n",
      "api-inferencia-7dc65cbb55-4vvkv   1/1     Running   0          135m\n",
      "api-inferencia-7dc65cbb55-kmxhz   1/1     Running   0          135m\n",
      "api-inferencia-7dc65cbb55-qblvk   1/1     Running   0          135m\n",
      "api-update-74d8cbcf86-sbt2j       1/1     Running   0          135m\n",
      "mlflow-6f49c6df8c-fljfb           1/1     Running   0          112m\n"
     ]
    }
   ],
   "source": [
    "# Check GCloud\n",
    "projects = !gcloud config get-value project\n",
    "set_project_command = f\"gcloud config set project {projects[0]}\"\n",
    "get_kubectl_command = \"gcloud container clusters get-credentials two-node-cluster --zone us-central1-a\"\n",
    "run_docker_command(set_project_command)\n",
    "run_docker_command(get_kubectl_command)\n",
    "run_docker_command(\"kubectl get pods\")\n",
    "# run_docker_command(\"gcloud config get-value project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2213eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE TEST\n",
    "locust_path = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/locust-testing.py\"\n",
    "check_kafka_lag_dir_path = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets\"\n",
    "loads = 10000\n",
    "locust_command = f\"locust -f {locust_path} --users {loads} --spawn-rate {loads} --headless --csv=result_testing\"\n",
    "run_docker_command(f\"cd {check_kafka_lag_dir_path} && nohup python3 check-kafka-lag.py > kafka_lag.log 2>&1 &\")\n",
    "run_docker_command(f\"cd /app/large-scale-online-learning/ && source ../.python-venv/bin/activate && nohup {locust_command} > locust.log 2>&1 &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812ce011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root        5564  0.0  0.0   4324   244 ?        S    17:08   0:00 bash -c cd /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets && nohup python3 check-kafka-lag.py > kafka_lag.log 2>&1 &\n",
      "root        5565  0.0  0.0  90664 11952 ?        Sl   17:08   0:00 python3 check-kafka-lag.py\n",
      "root        5637 50.0  0.0   4324  3296 ?        Ss   17:09   0:00 bash -c ps aux | grep check-kafka-lag.py\n",
      "root        5644  0.0  0.0   3528  1652 ?        S    17:09   0:00 grep check-kafka-lag.py\n",
      "root        5617  0.0  0.0   4324  2056 ?        S    17:08   0:00 bash -c cd /app/large-scale-online-learning/ && source ../.python-venv/bin/activate && nohup locust -f /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/locust-testing.py --users 10000 --spawn-rate 10000 --headless --csv=result_testing > locust.log 2>&1 &\n",
      "root        5618  103  2.6 2201812 867144 ?      Rl   17:08   0:42 /root/venv/bin/python3 /root/venv/bin/locust -f /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/locust-testing.py --users 10000 --spawn-rate 10000 --headless --csv=result_testing\n",
      "root        5645 50.0  0.0   4324  3464 ?        Ss   17:09   0:00 bash -c ps aux | grep locust-testing.py\n",
      "root        5652  0.0  0.0   3528  1568 ?        S    17:09   0:00 grep locust-testing.py\n"
     ]
    }
   ],
   "source": [
    "# check whether the script is running\n",
    "run_docker_command(\"ps aux | grep check-kafka-lag.py\")\n",
    "run_docker_command(\"ps aux | grep locust-testing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill the process if it is running\n",
    "run_docker_command(\"pkill -f check-kafka-lag.py\")\n",
    "run_docker_command(\"pkill -f locust-testing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e6ecf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755252960.2039907,2428\n",
      "1755253023.9973176,1837\n",
      "1755253087.9653482,934\n",
      "1755253151.6806755,150\n",
      "1755253215.071712,0\n",
      "1755253278.1660268,0\n",
      "1755253341.2906547,0\n",
      "1755253404.3765507,0\n",
      "1755253467.6990356,0\n",
      "1755253530.9016178,0\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF KAFKA LAG LOG IS GENERATED\n",
    "run_docker_command(\"cat /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv | tail -n 10\")\n",
    "\n",
    "# clean kafka lag log\n",
    "# run_docker_command(\"rm -rf /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dd94485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST     /predict                                                                      368286  3901(1.06%) |  16087       4  147961   8800 |  568.80        0.00\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated                                                                    368286  3901(1.06%) |  16087       4  147961   8800 |  568.80        0.00\n",
      "\n",
      "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "POST     /predict                                                                      370188  3901(1.05%) |  16124       4  147961   8800 |  503.20        0.00\n",
      "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated                                                                    370188  3901(1.05%) |  16124       4  147961   8800 |  503.20        0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF LOCUST LOGS ARE GENERATED\n",
    "run_docker_command(\"cat /app/large-scale-online-learning/locust.log | tail -n 10\")\n",
    "\n",
    "# Clean up locust log\n",
    "# run_docker_command(\"rm -rf /app/large-scale-online-learning/locust.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebcb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Kubernetes pod is working\n",
    "# update_pods = !kubectl get pods | findstr /R \"upd\"\n",
    "# !kubectl exec {update_pods[0].split()[0]} -- ls\n",
    "# !kubectl exec {update_pods[0].split()[0]} -- cat message_log.csv\n",
    "\n",
    "# inference_pods = !kubectl get pods | findstr /R \"inf\"\n",
    "# if inference_pods:\n",
    "#     print(f\"Inference pod found: {inference_pods[0]}\")\n",
    "#     !kubectl exec {inference_pods[0].split()[0]} -- ls\n",
    "#     !kubectl exec {inference_pods[0].split()[0]} -- cat message_log.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK Resource Usage of Load Testing VM\n",
    "!gcloud compute ssh load-testing-instance --zone=us-central1-a --command=\"top -b -n 1 | head -n 20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40249063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET VARIABLES\n",
    "with open(\"etc/variables.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            globals()[key] = value\n",
    "\n",
    "experiment = eval(experiment) if isinstance(experiment, str) else experiment\n",
    "if isinstance(experiment, list):\n",
    "    experiment_name = experiment[0]\n",
    "    experiment_file = experiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3190d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendation: To check for possible causes of SSH connectivity issues and get\n",
      "recommendations, rerun the ssh command with the --troubleshoot option.\n",
      "\n",
      "gcloud compute ssh load-testing-instance --project=fast-learner-project --zone=us-central1-a --troubleshoot\n",
      "\n",
      "Or, to investigate an IAP tunneling issue:\n",
      "\n",
      "gcloud compute ssh load-testing-instance --project=fast-learner-project --zone=us-central1-a --troubleshoot --tunnel-through-iap\n",
      "\n",
      "ERROR: (gcloud.compute.ssh) [C:\\Users\\muhammad.aulia\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\sdk\\plink.exe] exited with return code [1].\n"
     ]
    }
   ],
   "source": [
    "# Stop all running Python Load Testing scripts\n",
    "run_docker_command(\"pkill -f check-kafka-lag.py\")\n",
    "run_docker_command(\"pkill -f locust-testing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9848b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET INFERENCE RESULTS\n",
    "inference_pods = !kubectl get pods | findstr /R \"inf\"\n",
    "if inference_pods:\n",
    "    filenames = !kubectl exec {inference_pods[0].split()[0]} -- ls | findstr /R \"load_model\"\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    !kubectl exec {inference_pods[0].split()[0]} -- tar -czf /app/{filename}.tar.gz /app/{filename}\n",
    "    experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\inference-results\"\n",
    "    !mkdir {experiment_folder}\n",
    "    !kubectl cp default/{inference_pods[0].split()[0]}:/app/{filename}.tar.gz {experiment_folder}/{filename}.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fc7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "\n",
      "for_auc.tar.gz            | 32 kB |  32.0 kB/s | ETA: 00:04:28 |   0%\n",
      "for_auc.tar.gz            | 64 kB |  64.0 kB/s | ETA: 00:02:13 |   0%\n",
      "for_auc.tar.gz            | 736 kB | 368.0 kB/s | ETA: 00:00:21 |   8%\n",
      "for_auc.tar.gz            | 1504 kB | 501.3 kB/s | ETA: 00:00:14 |  17%\n",
      "for_auc.tar.gz            | 2432 kB | 608.0 kB/s | ETA: 00:00:10 |  28%\n",
      "for_auc.tar.gz            | 3136 kB | 627.2 kB/s | ETA: 00:00:08 |  36%\n",
      "for_auc.tar.gz            | 4032 kB | 672.0 kB/s | ETA: 00:00:06 |  46%\n",
      "for_auc.tar.gz            | 4768 kB | 681.1 kB/s | ETA: 00:00:05 |  55%\n",
      "for_auc.tar.gz            | 5664 kB | 708.0 kB/s | ETA: 00:00:04 |  65%\n",
      "for_auc.tar.gz            | 6432 kB | 714.7 kB/s | ETA: 00:00:03 |  74%\n",
      "for_auc.tar.gz            | 7296 kB | 729.6 kB/s | ETA: 00:00:01 |  84%\n",
      "for_auc.tar.gz            | 8032 kB | 730.2 kB/s | ETA: 00:00:00 |  93%\n",
      "for_auc.tar.gz            | 8610 kB | 782.8 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    }
   ],
   "source": [
    "# GET UPDATE RESULTS\n",
    "pods = !kubectl get pods | findstr /R \"upd\"\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /app/for_auc.tar.gz /app/for_auc.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /app/message_log.tar.gz /app/message_log.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /app/model_upload_latency.tar.gz /app/model_upload_latency.csv\n",
    "experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\update-results\"\n",
    "!mkdir {experiment_folder}\n",
    "run_docker_command(f\"kubectl cp default/{pods[0].split()[0]}:/app/for_auc.tar.gz app/for_auc.tar.gz\")\n",
    "copy_docker_file_to_local(\"/app/for_auc.tar.gz\", f\"{experiment_folder}/for_auc.tar.gz\")\n",
    "run_docker_command(f\"kubectl cp default/{pods[0].split()[0]}:/app/message_log.tar.gz app/message_log.tar.gz\")\n",
    "copy_docker_file_to_local(\"/app/message_log.tar.gz\", f\"{experiment_folder}/message_log.tar.gz\")\n",
    "run_docker_command(f\"kubectl cp default/{pods[0].split()[0]}:/app/model_upload_latency.tar.gz app/model_upload_latency.tar.gz\")\n",
    "copy_docker_file_to_local(\"/app/model_upload_latency.tar.gz\", f\"{experiment_folder}/model_upload_latency.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1ae74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this export KUBECTL_REMOTE_COMMAND_WEBSOCKETS=false on Windows\n",
    "!set KUBECTL_REMOTE_COMMAND_WEBSOCKETS=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check file size in /app\n",
    "!kubectl exec {pods[0].split()[0]} -- ls -lh /app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31335b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK MFLOW REPO\n",
    "# !kubectl exec {pods[0].split()[0]} -- ls /mlartifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a6e0dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "tar: Removing leading `/' from member names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "tar: Removing leading `/' from member names\n"
     ]
    }
   ],
   "source": [
    "# GET MLFLOW RESULT\n",
    "pods = !kubectl get pods | findstr /R \"mlflow\"\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /mlartifacts/mlflow-results.tar.gz /mlartifacts/1\n",
    "!kubectl exec {pods[0].split()[0]} -- tar -czf /mlartifacts/mlflow-db.tar.gz /mlartifacts/mlflow.db\n",
    "experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\mlflow-results\"\n",
    "!kubectl cp default/{pods[0].split()[0]}:/mlartifacts/mlflow-results.tar.gz {experiment_folder}/mlflow-results.tar.gz\n",
    "!kubectl cp default/{pods[0].split()[0]}:/mlartifacts/mlflow-db.tar.gz {experiment_folder}/mlflow-db.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "626635b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "result_testing_stats_hist | 32 kB |  32.0 kB/s | ETA: 00:00:04 |  19%\n",
      "result_testing_stats_hist | 128 kB |  64.0 kB/s | ETA: 00:00:00 |  78%\n",
      "result_testing_stats_hist | 164 kB |  82.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "result_testing_exceptions | 0 kB |   0.0 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "result_testing_failures.c | 0 kB |   0.3 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "result_testing_stats.csv  | 0 kB |   0.6 kB/s | ETA: 00:00:00 | 100%\n",
      "\n",
      "kafka_lag_log.csv         | 0 kB |   0.5 kB/s | ETA: 00:00:00 | 100%\n"
     ]
    }
   ],
   "source": [
    "# GET LOAD TESTING RESULT\n",
    "docker_path = \"/app/large-scale-online-learning/\"\n",
    "docker_path_kafka_log = \"/app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/\"\n",
    "experiment_folder = f\"experiment-results\\\\{experiment_name + '-' + str(loads) + 'VU'}\\\\load-test-results\"\n",
    "\n",
    "import os\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_stats_history.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_exceptions.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_failures.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path}result_testing_stats.csv\", experiment_folder)\n",
    "copy_docker_file_to_local(f\"{docker_path_kafka_log}kafka_lag_log.csv\", experiment_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c3989ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/app/message_log.csv': No such file or directory\n",
      "command terminated with exit code 1\n",
      "rm: cannot remove '/app/model_upload_latency.csv': No such file or directory\n",
      "command terminated with exit code 1\n",
      "rm: cannot remove '/app/for_auc.csv': No such file or directory\n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "# Delete all copied files in kubernetes\n",
    "# pods = !kubectl get pods | findstr /R \"upd\"\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/message_log.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/model_upload_latency.csv\n",
    "!kubectl exec {pods[0].split()[0]} -- rm /app/for_auc.csv\n",
    "pods = !kubectl get pods | findstr /R \"inf\"\n",
    "for pod in pods:\n",
    "    # Delete all files matching load_model*.csv at the end of the file in /app, suppress error if not found\n",
    "    !kubectl exec {pods[0].split()[0]} -- bash -c \"rm -f /app/load_model*.gz\"\n",
    "    !kubectl exec {pod.split()[0]} -- bash -c \"rm -f /app/load_model*.csv\"\n",
    "    !kubectl exec {pod.split()[0]} -- bash -c \"rm -f /app/api_inference_*.log\"\n",
    "run_docker_command(\"rm -rf /app/large-scale-online-learning/MLOps-Architecture/Serialization_Datasets/kafka_lag_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do ls to one of the pod\n",
    "pods = !kubectl get pods | findstr /R \"inf\"\n",
    "for pod in pods:\n",
    "    !kubectl exec {pod.split()[0]} -- ls /app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
